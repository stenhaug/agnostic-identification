
```{r}
out <- read_rds(here("simstudy/finalsim.rds"))

# A good check - 
# out %>%
#   mutate(bigsim = bigsim %>% map(~ select(., foc_mean))) %>%
#   unnest(bigsim) %>%
#   ggplot(aes(x = n_dif_items, y = foc_mean)) +
#   geom_point(alpha = 0.45)

n_items <- 12

final_model_to_anchor_summary <- function(final_model, n_dif_items){
    coef_1f(final_model)$items %>%
        mutate(
            true_anchor =
                c(
                    rep("no dif", n_items - n_dif_items),
                    rep("some dif", ceiling(n_dif_items / 2)),
                    rep("high dif", floor(n_dif_items / 2))
                )
        ) %>%
        group_by(true_anchor) %>%
        summarize(selected = sum(anchor), total = n())
}

getfull <- function(one){
    one %>%
        select(foc_mean, ends_with("mod"), -intuitive_mod) %>%
        mutate(replication = row_number()) %>%
        gather(method, model, -replication, -foc_mean) %>%
        mutate(
            n_anchors = model %>% map_int(~ sum(coef_1f(.)$items$anchor)),
            est_foc_mean = model %>% map_dbl(~ coef_1f(.)$ability$b_foc[1])
        ) %>%
        filter(n_anchors != 0 | method %in% c("gini_mod", "minbc_mod")) %>% 
    mutate(resid = est_foc_mean - foc_mean)
}

what <- 
  out %>%
    mutate(achievegap = bigsim %>% map(getfull)) %>%
    select(-bigsim) %>%
    unnest(achievegap)

one_to_anchors <- function(one, n_dif_items){
    one %>%
        select(ends_with("mod"), -intuitive_mod, -gini_mod, -minbc_mod) %>%
        mutate(replication = row_number()) %>%
        gather(method, model, -replication) %>%
        mutate(anchor_summary = model %>% map(final_model_to_anchor_summary, n_dif_items)) %>%
        select(-replication, -model) %>%
        unnest(anchor_summary) %>%
        group_by(method, true_anchor, total) %>%
        summarize(mean_selected = mean(selected)) %>%
        ungroup()
}

temp <-
    out %>%
    mutate(anchors = map2(bigsim, n_dif_items, one_to_anchors)) %>%
    select(-bigsim) %>%
    unnest(anchors)

false_anchor <- 
  temp %>%
    filter(true_anchor != "no dif") %>% 
    group_by(n_dif_items, method) %>%
    summarize(total = sum(total), mean_selected = sum(mean_selected)) %>%
    ungroup() %>% 
    mutate(rate = mean_selected / total) %>% 
    mutate(
      method = 
        factor(
          method,
          levels = c("AOAA_OAT_mod", "cluster_mod", "AOAA_AS_mod", "AOAA_mod"),
          labels = c("AOAA-OAT", "EMC", "AOAA-AS", "AOAA")
        )
    ) %>% 
    filter(method != "EMC") 

anchor_hit <- 
  temp %>%
    filter(true_anchor == "no dif") %>%
    mutate(rate = mean_selected / total) %>% 
    mutate(
        method = 
          factor(
            method,
            levels = c("AOAA_OAT_mod", "cluster_mod", "AOAA_AS_mod", "AOAA_mod"),
            labels = c("AOAA-OAT", "EMC", "AOAA-AS", "AOAA")
          )
    ) %>%  
    filter(method != "EMC")
```

The verbal aggression data provided an example of how AGI methods can lead to different conclusions. However, being real-world data, we cannot know the data generating processes underlying the differences that we observed. And, as a consequence, we could not adjudicate between methods. To illustrate the AGI methods' performance when the data generating model is known, we conducted a simulation study. Within the simulation study, we were able to measure the performance of each AGI method, but it's critical to note out at the outset that a simulation study can only conclude that a method is best under specific conditions, not in general.

The majority of DIF simulation studies in the literature generate data by simply altering the item easiness parameters for the focal group (e.g., setting the focal group's item easiness parameter to the reference group's item easiness parameter minus a constant). As described in the appendix, this setup views DIF as a property of the item that equally impacts all students from the same group. We find it more realistic to think of DIF as the result of each item tapping into a secondary ability to a varying degree. For example, @ackerman1992didactic describe a scenario in which some items on a math test depend on both a student's math ability (the target ability) and the student's verbal ability (the nuisance ability).

## Data Generating Model

Our simulation study was inspired by @walker2017using who conducted a series of simulations to determine how high the correlation between the target and nuisance abilities must be for DIF to undetectable. Unlike @walker2017using, we used a noncompensatory item response model where an individual cannot compensate for being low on the nuisance ability by being relatively high on the target ability. @ackerman1992didactic's scenario of a math test may be an example of a case where a noncompensatory model is realistic: Perhaps a student's math ability is largely irrelevant when they encounter a word problem that they cannot parse. In particular, we generated item responses using an adapted version of Sympson’s -@sympson1978model noncompensatory item response model in which
\begin{equation}
\text{Pr}(y_{ij} = 1 | \theta_i, \eta_i, a_{\eta j}) = \sigma(\theta_i + b_{\theta j}) \cdot \sigma(a_{\eta j}\eta_i + b_{\eta j})
\end{equation}
where $\theta_i$ and $\eta_i$ is individual $i$'s target ability and nuisance ability, respectively; $a_{\eta j}$ is item $j$'s loading on nuisance ability; and $b_{\theta j}$ and $b_{\eta j}$ are item $j$'s target easiness and nuisance easiness, respectively [@demars2016partially].

Each test had 12 items, and we used five conditions corresponding to two, three, four, five, and six DIF items. For items without DIF (i.e., that don't depend on the nuisance ability), we set $b_{\eta j} = \infty$ and $b_{\theta j} = 0$ which resulted in the model reducing to $\text{Pr}(y_{ij} = 1) = \sigma(\theta_i)$. For items with DIF (i.e., that depend on the nuisance ability), we chose relatively high item easiness parameters—$b_{\theta j} = 2$ and $b_{\theta j} = 2$—so that the overall probability after multiplying the two sigmoid functions together was similar for items with and without DIF. This choice resulted in the model reducing to $\text{Pr}(y_{ij} = 1) = \sigma(\theta_i + 2) \cdot \sigma(a_{\eta j}\eta_i + 2)$. For items with DIF, $a_{\eta j}$ was calculated by adapting Ackerman's -@ackerman1994using angle equation as described in @walker2017using:
\begin{equation}
\angle_j = \arccos \dfrac{1}{1 + a_{\eta j}^2}.
\end{equation}
An item's angle measures the relative loading of the item on the two dimensions. The lower the angle, the more the item loads on target ability as compared to nuisance ability, which corresponds to less DIF. An angle of 45$^\circ$ indicates that the item loads equally on the target ability and nuisance ability. We were interested in specifying the angle of an item, so the relevant equation became
\begin{equation}
a_{2j} = \sqrt{\dfrac{1 - \cos(\angle_j)^2}{\cos(\angle_j)^2}}.
\end{equation}
For DIF items, we set $a_{\eta j}$ based on angles with equal intervals between 20$^\circ$ and 60$^\circ$. For example, for a test with three DIF items, the angles are 20$^\circ$, 40$^\circ$, and 60$^\circ$.

We conducted 100 replications for each of the five conditions. In each replication, we simulated 10,000 students with half coming from each of the reference and focal groups. For students from the reference group, target ability and nuisance ability were drawn from the two-dimensional normal distribution with mean [$\mu_\theta^\text{ref} = 0$, $\mu_\eta^\text{ref} = 0$] and covariance matrix $\begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}$. Abilities for students from the focal group were drawn using the same covariance matrix, but with means [$\mu_\theta^\text{foc} = -0.5$, $\mu_\eta^\text{foc} = -1$]. In summary, the focal group had a lower mean target ability and a lower mean nuisance ability; it’s the goal of a DIF detection process to result in unbiased measurement of the target ability.

Figure \ref{fig:difmap} provides intuition about the data generating model by showing the relationship between $\theta_i$ and $\text{Pr}(y_{ij} = 1)$ (i.e., item characteristic curves) with $\eta_i$ set to the group mean for a test with six items with DIF. The items are ordered by the amount of DIF such that $\angle_{j = 7} = 20^\circ$ up to $\angle_{j = 12} = 60^\circ$. In essence, the two groups had the same relationship with items one through six as those items did not load on $\eta_i$. The last six items contained increasing amounts of DIF against the focal group as those items loaded on $\eta_i$.

```{r difmap, fig.cap = 'The relationship between target ability and probability of correct response for a 12-item test where the last 6 items contain DIF. Nuisance ability is fixed to the group mean. The reference group has the same item characteristic curve for each item; the focal group has lower probabilities of correct responses the more the item requires the nuisance ability.', out.width="100%", warning = FALSE, message = FALSE}
out$bigsim[[5]]$pars[[1]] %>% 
  graph_pars_noncompensatory(0, -1) +
  scale_y_continuous(labels = scales::percent) +
  labs(
    x = latex2exp::TeX("$\\theta_i$"),
    y = latex2exp::TeX("$Pr(y_{ij} = 1)$")
  )
```

We now describe the analysis methods that we took. We sequence these in two steps—first, inspect a GLIMMER and second, execute and compare the results from multiple AGI methods—so as to mirror our recommended DIF detection process for an analyst confronted with a single data set. Each of the analysis methods involve fitting the multigroup Rasch model (see equation \ref{eq:rasch}). Computing was done in R [@rcore], model fitting in the mirt R package [@chalmers2012mirt], and data wrangling/visualization in the suite of R packages known as the tidyverse [@tidy]. Code is available at *BLINDED*.

## Step 1: Create a GLIMMER

Figure \ref{fig:difmap} requires knowledge of the data generating model, and is therefore unavailable to the analyst. The next best thing is a GLIMMER, which allows the analyst to visualize the performance differences on each item. We suggest that the analyst begins DIF detection by looking at a GLIMMER. As an example, Figure \ref{fig:simemmlg} shows the GLIMMER for one replication using the same item parameters that generated Figure \ref{fig:difmap}. As a reminder, a GLIMMER is generated by fixing both group means to 0 so that all performance differences manifest in the free estimation of the item easiness parameters. As expected, $\tilde{d_j}$ is consistent for the first six items which do not contain DIF. Further, for the last six items, $\tilde{d_j}$ increases as $\angle_j$ increases. From inspecting the GLIMMER, the analyst might (correctly) assume that the first six items can be used as anchors. In general, the analyst should be reassured to by a GLIMMER that shows consistent differences across items.

```{r simemmlg, fig.height = 4, fig.cap = 'GLIMMER for one replication using the same item parameters as generated Figure \\ref{fig:difmap}. The six DIF-free items (items 1-6) show a constant performance difference. As expected, the other six items (items 7-12) show an increasingly large performance difference.', out.width="70%", warning = FALSE, message = FALSE}
out$bigsim[[5]]$intuitive_mod[[5]] %>% 
    mod_intuitive_to_draws_df() %>%
    draws_df_to_logit_plot() +
    labs(
          x = latex2exp::TeX("$\\tilde{d_j} = \\tilde{b_j}^{ref} - \\tilde{b_j}^{foc}$")
      )
```

## Step 2: Compare Multiple AGI Methods

After inspecting a GLIMMER, our second suggested step for DIF detection is to compare performance across multiple AGI methods. Of course, the analyst will typically be working with a single data set and will not know the true data generating model. Here, we implemented a simulation study version of this step by comparing the performance of each of the AGI methods across each of the replications. In particular, we applied each AGI method to find the method's identifying assumption. The method's identifying assumption was then used to fit a final model. We compared the performance of those final models according to the following outcomes.

**Anchor Item Detection:** For the methods that choose a set of anchor items, we looked directly at which anchor items were selected. An effective method should use most of the non-DIF items as anchors (the anchor hit rate) while avoiding using items with DIF as anchors (the DIF avoidance rate).

**Achievement Gap Residual:** An effective AGI method should lead to a final model that accurately estimates the difference between the reference group's mean target ability and the focal group's mean target ability. We refer to this quantity as the achievement gap. Recall that all models set $\mu_\theta^\text{ref} = 0$, so the achievement gap reduces to $\mu_\theta^\text{foc}$. The data-generating value of $\mu_\theta^\text{foc}$ is 0.5, but each replication, of course, includes sampling variability. To get at the heart of how well a method is doing, we calculated the achievement gap residual as the method's estimated achievement gap, $\hat\mu_\theta^\text{foc}$, minus the achievement gap estimated when using only the DIF-free items as anchors. In summary, this outcome measures a method’s ability to disentangle differences in target ability from nuisance ability at the group level. <!--ben says measurement error by theta over rank order-->

### Results

AOAA-OAT performed much better at correctly identifying anchor items than AOAA-AS and AOAA. The anchor item detection results are shown in Figure \ref{fig:anchorfalse}. As shown in the top row, AOAA-OAT remarkably included an average of over $90\%$ of DIF-free items in the anchor set regardless of the number of DIF items on the test. Conversely, AOAA-AS and AOAA had significant performance degradation for conditions with more DIF items. As shown in the bottom row, each method performed similarly in terms of DIF avoidance rate. By inspecting several of the replications we observed a pattern that makes sense given the GLIMMER's geometry (see Figure \ref{fig:simemmlg}): AOAA-OAT tended to remove the items with DIF one by one, starting with the item with the most severe DIF (the largest angle). The only mistake that AOAA-OAT frequently made was failing to flag the item with 20$^\circ$ of DIF in its last iteration. AOAA-AS and AOAA fared much worse: Both methods tended to result in an anchor set comprised exclusively of the items with 20$^\circ$ and 40$^\circ$ of DIF. In most replications, AOAA-AS stopped after one or two iterations, which explains why AOAA and AOAA-AS had similar results.

```{r anchorfalse, fig.height = 4, fig.cap = 'Performance rates across 100 replications for each AGI method and number of DIF items. Top row: AOAA-OAT nearly always chooses all of the non-DIF items as anchors (the anchor hit rate) while AOAA-AS and AOAA do much better for fewer DIF items. Bottom row: All of the methods perform similarly as far as avoiding including items with DIF in the anchor set (DIF avoidance rate). DIF avoidance rates are slightly lower for the two DIF item condition because the item with 20$^\\circ$ of DIF was frequently incorrectly included in the anchor set (and it was one of only two items with DIF in this condition).', warning = FALSE, message = FALSE}
bind_rows(
  false_anchor %>% select(-total, -mean_selected) %>% mutate(rate = 1 - rate, label = "DIF Avoidance Rate"),
  anchor_hit %>% select(-total, -mean_selected, -true_anchor) %>% mutate(label = "Anchor Hit Rate")
) %>% 
  ggplot(aes(x = n_dif_items, y = rate)) +
  facet_grid(label ~ method, scales = "free_y") +
  geom_point(size = 3) +
  geom_path(alpha = 0.4) + 
  scale_y_continuous(labels = scales::percent, limits = c(0, 1))  +
  labs(x = "Number of DIF items", y = "Performance", color = "")
```

Ultimately, the goal of an item response model is often not to flag items containing DIF but rather to measure some property of the respondents without bias. One such possibility is the difference in group means which we have encapsulated in the achievement gap residual. Figure \ref{fig:achievegap} shows each method's performance on the achievement gap residual. AOAA-OAT was the clear winner. It performed nearly flawlessly for two, three, or four DIF items. Even when six of the 12 items on the test contained DIF, AOAA-OAT underestimated $\mu_\theta^\text{foc}$ by only 0.07 standard deviations on its worst replication. As expected, artificial DIF caused AOAA and AOAA-AS to exhibit problematic performance in conditions with more than three DIF items increased. MINBC and MAXGI performed similarly well with MINBC estimating the achievement gap with more precision but more bias than MAXGI, especially for tests with more than four DIF items. We hypothesize that MINBC's susceptibility to bias results from considering every item. AOAA-OAT, on the other hand, completely disregards an item once it's removed from the anchor set.

```{r achievegap, fig.cap = 'Achievement gap residual distributions across 100 replications for each AGI method and number of DIF items.', fig.height = 4, warning = FALSE, message = FALSE}
what %>% 
  mutate(
    method = 
      factor(
        method,
        levels = c("AOAA_OAT_mod", "AOAA_AS_mod", "AOAA_mod", "cluster_mod", "gini_mod",  "minbc_mod"),
        labels = c("AOAA-OAT", "AOAA-AS", "AOAA", "EMC", "MAXGI",  "MINBC")
      )
  ) %>% 
  filter(method != "EMC") %>% 
  ggplot(aes(x = resid, y = as.factor(n_dif_items))) +
  ggridges::geom_density_ridges() +
  facet_wrap(~ method) +
  coord_flip() +
  geom_vline(xintercept = 0, linetype = "dashed", color = "chartreuse4", size = 0.5) +
  labs(
      y = "Number of DIF items",
      x = latex2exp::TeX("Achievement gap residual")
  )
```

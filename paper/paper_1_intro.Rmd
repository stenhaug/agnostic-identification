Measures from surveys and assessments are in widespread use throughout social and biomedical science. Education researchers use end-of-year assessments to measure educational opportunity across communities [@reardon2019validation], psychologists use surveys to understand personality [@vernon2014personality], and medical researchers develop symptoms questionnaires that are widely used by clinicians [@amtmann2010development]. Such data has a general structure: individuals give categorical responses to a set of questions. The use of such data has a general goal: to better understand individuals' specific abilities through the measurement of unobservable latent variables rather than relying solely on observations [@borsboom2006attack].

A popular paradigm in service of this goal is item response theory (IRT) [@hambleton1991fundamentals]. In IRT, each individual’s response to each item on a measurement instrument is a function of the individual's latent variables and the item's parameters. The simplest IRT model, the Rasch model, specifies the probability of individual $i$ responding affirmatively to item $j$ as
\begin{equation}
\text{Pr}(y_{ij} = 1) = \sigma(\theta_i + b_j)
\end{equation}
where $\theta_i$ is the individual's latent variable (i.e., trait or ability), $b_j$ is the item's easiness, and $\sigma(x) = \dfrac{e^x}{1 + e^x}$ is the standard logistic function [@thissen1986taxonomy].

Frequently, categorical variables accompany item response data. As one example, the gender of the test-taker is typically collected as part of the administration of college entrance exams [@casey1995influence; @cai2019gender]. Other common "group" variables are male-female, high-low socioeconomic status, and rural-urban; in the general case they are referred to as the reference ("ref") and focal ("foc") group [@holland1986differential]. With a group variable, an improved measurement model might allow for the possibility of separate parameters for each group. In particular, the multigroup Rasch model allows the probability of individual $i$ responding affirmatively to item $j$ to vary as a function of individual $i$'s group membership,
\begin{equation}
\label{eq:rasch}
\text{Pr}(y_{ij} = 1) = \sigma(\theta_{i} + b_j^\text{group}).
\end{equation}
The notation $b_j^{\text{group}}$ indicates that each item has a separate easiness parameter for each group: $b_j^{\text{ref}}$ for persons in the reference group and $b_j^{\text{foc}}$ for persons in the focal group. The multigroup model allows for the possibility of differential item functioning (DIF); an item that contains DIF functions differently across groups and thus has the potential to cause bias [@camilli1994methods]. An item does not suffer from DIF when $b_j^{\text{ref}} = b_j^{\text{foc}}$. On the other hand, if, for example, $b_j^{\text{ref}} > b_j^{\text{foc}}$, the item contains DIF "against" the focal group. One way to think about DIF is that, conditional on ability, an item is DIF-free if persons have the same probability of responding correctly regardless of their group membership.^[This is most easily seen by graphing the item characteristic curves (the mapping of ability to probability of correct response) for each group and observing that they overlap.]

Items that contain DIF can invalidate the entire measurement process. As one example, @pae2006examining found that 22 out of 33 items from the English reading portion of a Korean college entrance exam contained DIF across gender. They concluded that the cumulative effect of these 22 items significantly contaminates test-level scores, potentially leading to unfair admission decisions. As such, effective DIF detection methods are crucial to the field of measurement, and psychometricians have long been in search of effective DIF detection methods [@millsap2012statistical]. DIF detection methods typically test the hypothesis that $b_j^{\text{ref}} = b_j^{\text{foc}}$. The most common is to use a likelihood ratio test (LRT) to compare the baseline model—where $b_j^{\text{ref}}$ and $b_j^{\text{foc}}$ are constrained to be equal—to a more flexible model where that constraint is removed [@thissen1993detection]. Critically, the item parameters for all of the other items are usually required to be equal across groups. As another example, a more recent and complex approach, the "lasso DIF method", begins with a baseline model where every item parameter is allowed to vary across groups [@magis2015detection]. The final model is found by lasso penalizing the baseline model such that a model where some items have equal parameters across groups is obtained. Critically, each person's sum score is used as the estimate of their latent ability throughout this process.

As is common, both of these DIF detection methods are based exclusively on internal criteria. As a result, they use the circular logic of looking for DIF items while assuming other items do not contain DIF [@camilli1994methods]. The LRT DIF method makes this assumption explicitly. The lasso DIF method make this assumption more subtly by using the sum score—which would be contaminated by DIF items—as an estimate of ability. Researchers have noticed this circularity, but have mostly described it indirectly by pointing out inflated type I errors in simulation studies [@stark2006detecting]. @andrich2012real refer to items incorrectly flagged as having DIF due to other items containing DIF as being caused by "artificial DIF." We argue that type I errors and the artificial DIF that can cause them are more clearly seen as consequences of an identification problem at the root of DIF detection. This identification problem may be easy to overlook, but solving it is no easy task. In fact, we see it as the antagonist of any DIF detection method and name it accordingly: the Fundamental DIF Identification Problem.

## The Fundamental DIF Identification Problem

Thus far, we have focused on item parameters, but ability parameters must be determined as well.^[As is common, we use marginal maximum likelihood estimation (MMLE) in which case it's only the group mean abilities, not the individual abilities, that are estimated in model fitting [@bock1981marginal].] The multigroup Rasch model typically assumes that persons from the reference group are distributed ${\theta_i}^{\text{ref}} \sim N(0,1)$ and persons from the focal group are distributed ${\theta_i}^{\text{foc}} \sim N(\mu^{\text{foc}}, \sigma^{\text{foc}^2})$. Setting the mean and variance for the reference group abilities to 0 and 1 simply determines the scale. The Fundamental DIF Identification Problem is that the focal group mean ability, $\mu^{\text{foc}}$, must be determined along with $b_j^{\text{ref}}$ and $b_j^{\text{foc}}$ for each item. A model cannot freely estimate all of these parameters (i.e., the model is under-identified). To make this lack of identifiability concrete, consider that the model has no way to disentangle the difference between (a) the focal group having higher ability and (b) every item containing bias against the reference group. ^[Mathematically, all IRT models with $\hat\mu^{\text{foc}} + c$ and $\hat b_j^{\text{ref}} - \hat b_j^{\text{foc}} + c$ are equivalent for any value of $c$.]

What we're calling the Fundamental DIF Identification Problem is both acknowledged and frequently overlooked by the literature [@zumbo2007three]. We argue that it has largely been communicated in a way that doesn't fully capture its importance. @camilli1994methods describe it as the requirement that "parameters must be 'equated' or scaled in the same metric" [p. 62]. @hambleton1991fundamentals, @embretson2013item, and @millsap2012statistical describe it as the need to create a common scale for linking across groups. As one example of the Fundamental DIF Identification Problem being overlooked, @cooke2005assessing concluded that a psychopathy instrument used for criminal risk assessment contained significant bias against North Americans as compared to Europeans. @bolt2007score pointed out that they made the mistake of beginning DIF detection with an unidentified model, and the exchange culminated in a legal battle that was picked up the New York Times [@carey2010academic].

The most trustworthy ways of addressing the Fundamental DIF Identification Problem use external information based on the context in which the item response data was gathered [@camilli1994methods]. For example, in a large randomized experiment, the groups might be determined equivalent at baseline, and the analyst can safely assume that $\mu^{\text{foc}} = \mu^{\text{ref}}$ on any instruments administered before the experiment's intervention. Or, an item like "2 + 2" might seem so innocuous that the analyst faithfully assumes that $b_j^{\text{foc}} = b_j^{\text{ref}}$ for that item.^[On the other hand, @angoff1993perspectives reports that test developers are often "confronted by DIF results that they cannot understand; and no amount of deliberation seems to help explain why some perfectly reasonable items have large DIF values" [p. 19]. It's unclear whether this observation indicates that seemingly innocuous items sometimes contain DIF or if it indicates a more basic failure of the DIF detection method all-together.] In the equating literature, the former is a "non-common item random groups" design and the latter is a "common-item nonequivalent groups" design [@cook1987problems; @topczewski2013comparison].^[With one of these assumptions in hand, the remainder of DIF detection is straightforward: each of the other items can be checked for DIF using well-validated methods such as a likelihood ratio test (LRT), which we use throughout this paper [@thissen1993detection].] However, in most cases the analyst is not in a position to make one of these assumptions. The multigroup model is unidentified and the analyst has no knowledge with which to make an identifying assumption; it in this sense that we say they are "agnostic" about how to resolve the Fundamental DIF Identification Problem. The analyst has nothing to hold onto so to speak. We refer to any method that the analyst turns to in this case as an "agnostic identification" (AGI) method, as opposed to the more general title of DIF detection method.^[To be sure, we refer to overcoming The Fundamental Problem of DIF without any a priori assumptions as AGI. DIF detection, on the other hand, describes the complete process. In this way, (we argue that) AGI is (perhaps the most important) part of DIF detection.]

At present "little evidence is available to guide applied researchers through the process" of DIF detection [@lopez2009effects, p. 252]. As a result, too-often DIF methods in applied research are at best ad hoc and at worst incorrect. As an example of the former, purification—the process of applying DIF methods in multiple iterations as opposed to in one-shot—is inconsistently used: Some recent applied DIF studies use it [e.g, @teresi2009analysis; @hagquist2017recent] while others do not [e.g., @dejosephcapturing; @sheldrick2019establishing; @crins2019differential; @lopez2020social]—and most do not give this potentially influential decision more than a sentence of consideration. Further, the majority of applied DIF studies neither acknowledge the assumptions on which their conclusions rest nor consider the possibility that their DIF detection process may not have worked properly.^[An outcome that we believe to be more likely the greater number of DIF items found. For example, it is not terribly uncommon for a study to conclude that nearly half of an instrument's items contain DIF.] In response, our aim is to help the analyst responsibly overcome the Fundamental DIF Identification Problem. To this end, we suggest three best practices for DIF detection: (1) descriptively examine raw item response data for potential DIF, (2) use and compare results from multiple DIF detection methods, and (3) report results as being dependent on the assumptions of the DIF detection methods used. These best practices are summarized in Table \ref{tab:suggest}, and we expand on each of them throughout the remainder of the paper.

\begin{table}[h]
\caption{Three best practices for DIF detection}
\centering
\begin{tabular}{|p{6cm}|p{9cm}|}
\toprule
 \textbf{Practice} & \textbf{Example} \\\midrule
 1. Descriptively examine raw item response data for potential DIF & By looking at a graph of raw item response data, the analyst realizes that the Fundamental DIF Identification Problem is challenging for their data.  \\\hline
 2. Use and compare results from multiple DIF detection methods & By implementing multiple DIF detection methods, the analyst recognizes that their conclusions will change depending on the method that they use. \\\hline
 3. Report results as being dependent on the assumptions of the DIF detection methods used & In presenting results, the analyst makes clear that their conclusions hinge on the assumptions of the DIF method used. \\ 
\bottomrule
\end{tabular}
\label{tab:suggest}
\end{table}

## Organization

We pause to describe the data that we use as a running example and then illustrate a novel graphical method for descriptively understanding potential DIF; based on reasoning introduced below, we suggest such analysis as the first step in any DIF detection process. We then survey a variety of existing and newly proposed AGI methods, with a focus on each method's underlying assumptions. We then provide a simulation study that demonstrates how each AGI method performs on data that mimics realistic conditions. We end by summarizing our recommendations for a robust DIF detection process. Our aim is that these contributions sum to a suggested process for DIF detection when AGI is the necessary starting point.

Throughout this paper, we continue to focus on the Rasch model which allows us to isolate the core intuition of DIF detection without the unnecessary complexities introduced in other cases (i.e., non-uniform DIF [@narayanon1996identification]). However, each of the methods that we describe is applicable to more flexible item response models. In the sections preceding the simulation study, we use the verbal aggression data collected by @vansteelandt2001formal as a running example so as to make our suggested process and descriptions of abstract methods concrete. Accordingly, we now briefly describe this data.

## Verbal Aggression Data

The verbal aggression data was collected from an undergraduate psychology course in order to understand differences in verbal aggressive behavior and its inhibition across sex [@vansteelandt2001formal]. It has become a classic data set in the DIF methods literature [e.g., @smits2004inhibition; @de2008random; @magis2010general]. The verbal aggression data includes item responses from 316 respondents (243 female and 73 male) and 24 items. As shown in Table \ref{table:verbal}, each of the $4 \cdot 3 \cdot 2 = 24$ items is the combination of one of four situations, one of three actions, and one of two types of responses. For example, the first item asks, "If a bus doesn't stop for you, would you want to curse?". A later item asks, "If a bus doesn't stop for you, would you actually curse?". The first item corresponds to "want" and the later item corresponds to "do." Consistent with most uses of the verbal aggression data in the literature, we ignore this nested structure when applying DIF methods.
\begin{table}[h]
\caption{The 24 items in the verbal aggression data come from crossing four situations, three actions, and two types. For example, the first row corresponds to the item that asks whether an individual would want to curse if a bus didn’t stop for them.}
\centering
\begin{tabular}{|p{5cm}|p{3cm}|p{3cm}|}
\toprule
 \textbf{Situation} & \textbf{Action} & \textbf{Type} \\\midrule
 Bus doesn't stop & Curse & Want \\\hline
 Miss a train & Shout & Do  \\\hline
 Grocery store closed & Scold & \\\hline
 Operator disconnects me &  & \\ 
\bottomrule
\end{tabular}
\label{table:verbal}
\end{table}

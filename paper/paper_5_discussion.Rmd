Validity hinges on measurement instruments being relatively free of DIF [@bryant2000assessing]. Such instruments need to be inspected for DIF so that we can be sure of the validity of the conclusions that we draw regardless of the group membership of each respondent. At the beginning of most DIF detection is a particularly difficult problem that we have referred to as the Fundamental DIF Identification Problem. This problem is typically tackled implicitly by DIF methods—during what we have called agnostic identification (AGI)—with the analyst sometimes lacking awareness of its existence. As a result, some applied DIF studies report certainly conclusions that rest on tenuous assumptions.

In other words, one of our main concerns is that the analyst can be lulled into a false sense of security: The analyst chooses a method, implements it, and then proceeds as if the method certainly resulted in a model that reflects some objective reality. The classic verbal aggression data provides a powerful example: Our results showed that different methods lead to significantly different conclusions. We argue that this is because the Fundamental DIF Identification Problem is unsolvable for the verbal aggression data. On the other hand, what other choice does the analyst have? Researchers have observed that their is little guidance for how to conduct AGI. 

In response, we suggest the following process for DIF detection. First, examine raw item response data for potential DIF. To this end, we introduced the GLIMMER. The GLIMMER combats this concern by presenting all information clearly to the analyst. The GLIMMER has the following advantages to this end: (a) By arbitrarily setting the group means equal, the GLIMMER remains agnostic to an identification assumption; (c) by using multiple imputations, the GLIMMER allows for understanding sampling variability/statistical significance; and (c) by using MMLE, the GLIMMER appropriately handles missing responses. Second, use and compare the results from multiple AGI methods. We demonstrated this process using the classic verbal aggression data, which highlighted just how sensitive DIF results can be to method. This is an important point given that most applied DIF studies use a single method and seem to give little consideration to the choice of that method. Third, understand the possible failure of DIF methods and report results as being contingent on assumptions.

Further, we conducted a simulation study where some items (i.e., the DIF-free items) loaded exclusively on target ability while other items (i.e., the DIF items) loaded on both target and nuisance ability. Mirroring our suggested DIF detection process, we showed an example of how a GLIMMER can give the analyst insight into the data generating process. We then compared each of the AGI methods in terms of their ability to correctly identify anchor items. AOAA-OAT exhibited superior performance, which is an important finding given that it is, to our knowledge, not used currently. We advocate for its widespread use as part of any DIF detection process. One drawback of the AOAA-OAT method is that it is computationally expensive. For example, finding three items containing DIF on a 12-item test requires fitting 46 item response models, and that number grows as either the test length or the number of items testing positive for DIF grows. However, we believe the computational costs are worthwhile. To increase AOAA-OAT's use, we recommend its implementation (perhaps as the default) in popular IRT software such as the mirt R package.

In closing, future work should test these methods' performance under a greater variety of data generating conditions. For example, changing the compensatory nature of the data generating model or adding additional nuisance ability dimensions. Finally, our work focused on the Rasch model, and it is of great interest to consider how these methods extend and perform when the goal is to detect and correct for DIF in the context of more flexible item response models.
